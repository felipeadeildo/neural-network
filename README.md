# Neural Network from Scratch

This repository is part of the coursework for the Introduction to Artificial Intelligence course of the [Novo Ensino Suplementar](https://novoensinossuplementar.com). The primary goal of this project is to build a neural network from scratch, implementing fundamental components such as neurons, activation functions, forward propagation, backpropagation, and gradient descent without relying on high-level libraries like TensorFlow or PyTorch. This project serves as a comprehensive guide to understanding the theoretical and mathematical foundations of neural networks, as well as their practical implementation.

## Table of Contents

- [Introduction](#introduction)
- [What is a Neural Network?](#what-is-a-neural-network)
- [Neurons and Layers](#neurons-and-layers)
- [Weights and Biases](#weights-and-biases)
- [Activation Functions](#activation-functions)
  - [Sigmoid](#sigmoid)
  - [ReLU](#relu)
  - [Softmax](#softmax)
- [Forward Propagation](#forward-propagation)
- [Cost Function](#cost-function)
- [Backpropagation](#backpropagation)
- [Gradient Descent](#gradient-descent)
- [Training the Neural Network](#training-the-neural-network)
- [Epochs](#epochs)
- [Example with Digit Recognition](#example-with-digit-recognition)
- [Conclusion](#conclusion)

---

### Introduction

Welcome to the "Neural Network from Scratch" project! The primary goal of this project is to build a neural network from the ground up, without using high-level machine learning libraries like TensorFlow or PyTorch. By doing so, we aim to provide a deep understanding of the inner workings of neural networks, including the theoretical and mathematical rigor behind them.

### What is a Neural Network?

A neural network is a computational model inspired by the human brain, composed of layers of interconnected nodes or neurons. These networks are capable of learning complex patterns and functions by adjusting the connections (weights) between neurons based on the data they are trained on. It's not a linear model once we have [activation functions](#activation-functions) to introduce non-linearity. Because of this, a neural network can learn more complex patterns than a linear model.

### Neurons and Layers

### Weights and Biases

_Coming soon_

### Activation Functions

_Coming soon_

#### Sigmoid

The sigmoid function maps any real-valued number into the range (0, 1), making it useful for binary classification.

$ \sigma(z) = \frac{1}{1 + e^{-z}} $

#### ReLU

The Rectified Linear Unit (ReLU) function is widely used in hidden layers due to its simplicity and efficiency.

$ \text{ReLU}(z) = \max(0, z) $

#### Softmax

The softmax function converts a vector of values into a probability distribution, often used in the output layer for multi-class classification.

$ \text{softmax}(z*i) = \frac{e^{z_i}}{\sum*{j} e^{z_j}} $

### Forward Propagation

_Coming soon_

### Cost Function

_Coming soon_

### Backpropagation

_Coming soon_

### Gradient Descent

_Coming soon_

### Training the Neural Network

_Coming soon_

### Epochs

_Coming soon_

### Example with Digit Recognition

_Coming soon_

### Conclusion

_Coming soon_
